{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11faf96-e095-41b1-8f64-a1f110f363bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Notebook Overview:\n",
    "- In this databricks notebook i am creating a ETL process layer that consists in reading data from the Bronze Folder, that has the data ingested from the API, i will perform some transformations, do some quality checks like look for duplicates and drop them, and finally save all of it into a silver table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "125ee000-4c7d-418d-9a6c-fc26fcbe7b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create the databases/schemas (each tool calls it in a different way)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02bff6d5-264d-47ce-81ea-9dd88c1075fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" CREATE DATABASE IF NOT EXISTS silver\"\"\")\n",
    "spark.sql(\"\"\" CREATE DATABASE IF NOT EXISTS gold\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6f64d0-3e02-49f0-9c40-85bd7dae49d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#     spark.sql(\"SHOW DATABASES\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c89b30-4eeb-492f-86ed-59756d5a29a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "import hashlib\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e93953bc-d999-4c47-b1cc-4895903a87ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read the data ingested in bronze layer"
    }
   },
   "outputs": [],
   "source": [
    "# Read JSON files from the Bronze layer\n",
    "df_bronze = spark.read.format(\"delta\").load(\"/mnt/files/bronze/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "774bb085-12dc-4bb8-8756-12702aa03fc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the schema"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"brewery_type\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"website_url\", StringType(), True),\n",
    "    StructField(\"updated_at\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116a8c94-a82b-4dc7-a2ba-8babd32094e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Cleaning + data transformation"
    }
   },
   "outputs": [],
   "source": [
    "# Cast longitude and latitude to DoubleType\n",
    "df_clean = df_bronze.withColumn(\"longitude\", col(\"longitude\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"latitude\", col(\"latitude\").cast(DoubleType()))\n",
    "\n",
    "# drops the rows with missing id\n",
    "df_clean = df_clean.na.drop(subset=[\"id\"])\n",
    "\n",
    "# Standardize text columns to lower case\n",
    "text_columns = [\"name\", \"brewery_type\", \"city\", \"state\", \"country\"]\n",
    "for col_name in text_columns:\n",
    "    df_clean = df_clean.withColumn(col_name, lower(col(col_name)))\n",
    "\n",
    "# masking sensitive data (like phone, user cpf, user uuid and so on)\n",
    "def hash_sensitive_data(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    return df.withColumn(column_name, sha2(col(column_name), 256))\n",
    "\n",
    "df_clean = hash_sensitive_data(df_clean, \"phone\")\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9226ec-431d-4249-9ea6-ea9837f37948",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data quality checks"
    }
   },
   "outputs": [],
   "source": [
    "#drops the duplicated rows\n",
    "df_clean = df_clean.dropDuplicates([\"id\"])\n",
    "\n",
    "#drops the rows that has the postal code out of the 'xxxxx-xxxx' format - testing for USA as an example\n",
    "usa_postal_code_pattern = \"^[0-9]{5}(-[0-9]{4})?$|^[0-9]{5}-?$\"  # USA format: xxxxx or xxxxx-xxxx\n",
    "\n",
    "# Filter rows with invalid postal codes for the United States\n",
    "df_invalid_postal = df_clean.filter(\n",
    "    (col(\"country\") == \"united states\") & ~col(\"postal_code\").rlike(usa_postal_code_pattern)\n",
    ")\n",
    "\n",
    "# Check for duplicates based on 'id'\n",
    "df_duplicates = df_clean.groupBy(\"id\").count().filter(\"count > 1\")\n",
    "if df_duplicates.count() > 0:\n",
    "    print(f\"Warning: i found: {df_duplicates.count()} Duplicates rows found based on 'id'. \")    \n",
    "\n",
    "#drops the duplicated rows\n",
    "df_clean = df_clean.dropDuplicates([\"id\"])\n",
    "\n",
    "# Validate 'postal_code' format\n",
    "if df_invalid_postal.count() > 0:\n",
    "    print(\"Warning: Invalid postal codes found.\")\n",
    "\n",
    "#filter and test for both phone and website rows NOT NULL\n",
    "df_clean = df_clean.filter(~(col(\"phone\").isNull() & col(\"website_url\").isNull()))\n",
    "\n",
    "df_no_contact_information = df_clean.filter((col(\"phone\").isNull() & col(\"website_url\").isNull()))\n",
    "                                            \n",
    "if df_no_contact_information.count() > 0:\n",
    "    print(f\"We have {df_no_contact_information.count()} registries that does not have a contact information\")\n",
    "\n",
    "\n",
    "# Log data quality issues\n",
    "data_quality_issues = df_duplicates.count() + df_invalid_postal.count() + df_no_contact_information.count()\n",
    "if data_quality_issues > 0:\n",
    "    raise Exception(\"We should not carry data with these quality checks not approved\")\n",
    "else:\n",
    "    fl_quality = 1 #setting a quality check flag in case we need in the future\n",
    "    print(\"Our data is good to proceed :)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3416d964-9000-4fa8-9e9b-4229453e162d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**saving the cleaned data**\n",
    "- if the table does not exists, perform the overwrite and writes a delta table\n",
    "- otherwise perform the merge statement, making sure that the 'id's will not repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "680ad42d-da70-438a-9f5e-b7a984bc95bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add timestamp load for silver layer"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn(\"ts_loadtimestamp_silver\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e47684-e54f-42e3-be81-39398c8072a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save it partitioned by ts_load from bronze ingestion"
    }
   },
   "outputs": [],
   "source": [
    "delta_table_path = '/mnt/files/silver/'\n",
    "df_clean.write.partitionBy(\"state\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ddff653-d387-4a30-8304-e08c05e35d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Overrall review for this notebook**:\n",
    "- this notebook performs some data cleaning such as: droping -> dupicated id's, postal codes not accordinly to a pattern (tested for USA postal codes, can be replicated to other countries), also droped rows that does not have contact information (both phone AND website_url).\n",
    "- performed a sensitive data masking for phone as an example, could have been for user name, last name, date of birth and so on.\n",
    "- this logic also protects the data load if the quality checks fail rasing an error that will be caught in datafactory monitoring\n",
    "- finally we partitioned the data by the load timestamp which is the usual approach in production, but since we only have this column after setting it up on the ingestion to bronze layer, we had to perform a similar approach when saving the data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3a43e9-885c-41a3-8131-d0f808c5781d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'OPTIMIZE budinworkspac.silver.silver_bewery')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_etl_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
