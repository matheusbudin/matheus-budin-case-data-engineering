{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a4a043-16d8-49ec-a54f-fe85040d8017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec275e96-0a4e-4bcd-aefa-1cb8a4200939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Initialize variables\n",
    "# 'total_pages' means = set the ammount of list chuncks to be fetched\n",
    "api_url = \"https://api.openbrewerydb.org/v1/breweries\"\n",
    "per_page = 50\n",
    "total_pages = 10 \n",
    "all_breweries_data = []\n",
    "\n",
    "# Fetch data from the API in chunks of 50 items\n",
    "for page in range(1, total_pages + 1):\n",
    "    params = {'per_page': per_page, 'page': page}\n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    # Check for successful response\n",
    "    if response.status_code == 200:\n",
    "        breweries_data = response.json()\n",
    "        all_breweries_data.extend(breweries_data)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from page {page}:\", response.status_code)\n",
    "        break\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"brewery_type\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"website_url\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Preprocess JSON to ensure consistency\n",
    "processed_data = [\n",
    "    {\n",
    "        \"id\": item.get(\"id\"),\n",
    "        \"name\": item.get(\"name\"),\n",
    "        \"brewery_type\": item.get(\"brewery_type\"),\n",
    "        \"street\": item.get(\"street\"),\n",
    "        \"city\": item.get(\"city\"),\n",
    "        \"state\": item.get(\"state\"),\n",
    "        \"postal_code\": item.get(\"postal_code\"),\n",
    "        \"country\": item.get(\"country\"),\n",
    "        \"longitude\": float(item[\"longitude\"]) if item.get(\"longitude\") else None,\n",
    "        \"latitude\": float(item[\"latitude\"]) if item.get(\"latitude\") else None,\n",
    "        \"phone\": item.get(\"phone\"),\n",
    "        \"website_url\": item.get(\"website_url\")\n",
    "    }\n",
    "    for item in all_breweries_data\n",
    "]\n",
    "\n",
    "# Convert preprocessed data to PySpark DataFrame\n",
    "df_brewery_api_fetch = spark.createDataFrame(processed_data, schema)\n",
    "\n",
    "# Add \"ts_load_timestamp\" column\n",
    "df_brewery_api_fetch = df_brewery_api_fetch.withColumn(\"ts_load_timestamp\", current_timestamp())\n",
    "\n",
    "# Count total number of records before removing duplicates\n",
    "total_records = df_brewery_api_fetch.count()\n",
    "\n",
    "# Identify duplicates based on 'id'\n",
    "df_duplicates = df_brewery_api_fetch.groupBy(\"id\").count().filter(\"count > 1\")\n",
    "duplicates_count = df_duplicates.count()\n",
    "\n",
    "# Show duplicates found\n",
    "print(f\"Number of duplicate IDs found: {duplicates_count}\")\n",
    "df_duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates\n",
    "df_brewery_api_fetch = df_brewery_api_fetch.dropDuplicates([\"id\"])\n",
    "\n",
    "# Count total number of records after removing duplicates\n",
    "final_records = df_brewery_api_fetch.count()\n",
    "dropped_records = total_records - final_records\n",
    "\n",
    "# Report the number of records dropped\n",
    "print(f\"Total records before removing duplicates: {total_records}\")\n",
    "print(f\"Total records after removing duplicates: {final_records}\")\n",
    "print(f\"Number of records dropped: {dropped_records}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a70dde-4879-4e64-a296-9651155fcf86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save the result in azure data lake gen 2 partitioned by State\n",
    "- deduplication will be done in the silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5678fe8d-ca2b-43d7-ad4d-4ace843a74ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_brewery_api_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9753e773-eb99-44ae-bca2-7efa0653acec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to the specified mount point as a Parquet file\n",
    "df_brewery_api_fetch.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"dbfs:/mnt/files/bronze/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c1c6b7-1505-4a66-9e9a-9ae3b14b5ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**apply Z order optimization only for tables - from silver layer and beyond**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_brewery_api_fetch_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
